{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, batch_size=32, max_iters=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.max_iters = max_iters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"Sigmoid function to map any real value to (0, 1)\"\"\"\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def loss(self, y_true, y_pred):\n",
    "       \n",
    "        m = len(y_true)\n",
    "        return - (1/m) * np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    \n",
    "    def initialize_weights(self, n_features):\n",
    "        \"\"\"Initialize the weights and bias\"\"\"\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward pass: compute predictions using sigmoid\"\"\"\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        y_pred = self.sigmoid(linear_model)\n",
    "        return y_pred\n",
    "    \n",
    "    def compute_gradient(self, X, y_true, y_pred):\n",
    "        \"\"\"Compute the gradients for weights and bias\"\"\"\n",
    "        m = X.shape[0]  # Batch size\n",
    "        dw = (1/m) * np.dot(X.T, (y_pred - y_true))  # Gradient for weights\n",
    "        db = (1/m) * np.sum(y_pred - y_true)         # Gradient for bias\n",
    "        return dw, db\n",
    "    \n",
    "    def update_weights(self, dw, db):\n",
    "        \"\"\"Update weights using gradient descent\"\"\"\n",
    "        self.weights -= self.learning_rate * dw\n",
    "        self.bias -= self.learning_rate * db\n",
    "    \n",
    "    def train(self, X, y):\n",
    "        \"\"\"Training the logistic regression model using mini-batch SGD\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        self.initialize_weights(n_features)\n",
    "        \n",
    "        for i in range(self.max_iters):\n",
    "            # Shuffle the data\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            X_shuffled = X[indices]\n",
    "            y_shuffled = y[indices]\n",
    "            \n",
    "            # Mini-batch training\n",
    "            for start in range(0, n_samples, self.batch_size):\n",
    "                end = start + self.batch_size\n",
    "                X_batch = X_shuffled[start:end]\n",
    "                y_batch = y_shuffled[start:end]\n",
    "                \n",
    "                # Forward pass\n",
    "                y_pred = self.forward(X_batch)\n",
    "                \n",
    "                # Compute gradients\n",
    "                dw, db = self.compute_gradient(X_batch, y_batch, y_pred)\n",
    "                \n",
    "                # Update weights and bias\n",
    "                self.update_weights(dw, db)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions on new data\"\"\"\n",
    "        y_pred = self.forward(X)\n",
    "        y_pred_class = [1 if i > 0.5 else 0 for i in y_pred]\n",
    "        return np.array(y_pred_class)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load Wisconsin Breast Cancer dataset from scikit-learn\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution in training (+ validation) set: [196 316]\n"
     ]
    }
   ],
   "source": [
    "class_counts_train_val = np.bincount(np.concatenate([y_train, y_val]))\n",
    "print(f\"Class distribution in training (+ validation) set: {class_counts_train_val}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d,e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8h/xh3wy4j12tb1sjbxcy4hc9wm0000gn/T/ipykernel_7884/3067541625.py:13: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Performance:\n",
      "Accuracy: 0.9474\n",
      "Precision: 1.0000\n",
      "Recall: 0.9268\n",
      "F1 Score: 0.9620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8h/xh3wy4j12tb1sjbxcy4hc9wm0000gn/T/ipykernel_7884/3067541625.py:13: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Performance:\n",
      "Accuracy: 0.9123\n",
      "Precision: 1.0000\n",
      "Recall: 0.8780\n",
      "F1 Score: 0.9351\n",
      "Test Set Performance:\n",
      "Accuracy: 0.8246\n",
      "Precision: 1.0000\n",
      "Recall: 0.7561\n",
      "F1 Score: 0.8611\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model1 = LogisticRegression(learning_rate=0.01, batch_size=16, max_iters=1000)\n",
    "model1.train(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred1 = model1.predict(X_test)\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred1)\n",
    "precision = precision_score(y_test, y_pred1)\n",
    "recall = recall_score(y_test, y_pred1)\n",
    "f1 = f1_score(y_test, y_pred1)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred1)\n",
    "precision = precision_score(y_test, y_pred1)\n",
    "recall = recall_score(y_test, y_pred1)\n",
    "f1 = f1_score(y_test, y_pred1)\n",
    "print(f\"Test Set Performance:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "model2 = LogisticRegression(learning_rate=0.001, batch_size=8, max_iters=1000)\n",
    "model2.train(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred2 = model2.predict(X_test)\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred2)\n",
    "precision = precision_score(y_test, y_pred2)\n",
    "recall = recall_score(y_test, y_pred2)\n",
    "f1 = f1_score(y_test, y_pred2)\n",
    "print(f\"Test Set Performance:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "model3 = LogisticRegression(learning_rate=0.0001, batch_size=32, max_iters=1000)\n",
    "model3.train(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred3 = model3.predict(X_test)\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred3)\n",
    "precision = precision_score(y_test, y_pred3)\n",
    "recall = recall_score(y_test, y_pred3)\n",
    "f1 = f1_score(y_test, y_pred3)\n",
    "print(f\"Test Set Performance:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the accuracy scores from using the test set on each of the 3 mode3ls, it appears that a batch size of 32 is the best gradient descent method, which makes intiutive sense because the more information you feed the model, in terms of data points to compute gradients, the more accurate the gradient descent will be. However, if the bath size goes too high, then the computational cost can become too costly. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
